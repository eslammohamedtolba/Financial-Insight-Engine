# ü§ñ Financial Insight Engine

A sophisticated RAG (Retrieval-Augmented Generation) system powered by a hybrid LLM architecture. This application features a locally-run, fine-tuned **Llama 3 8B** for expert financial answer generation and **Google's Gemini Pro** for advanced query analysis of SEC 10-K filings. 

This application provides intelligent analysis of the latest 10-K filings from five major technology companies (Apple, Microsoft, Google, Amazon, Meta). The system leverages a state-of-the-art hybrid LLM strategy, combining a specialized local model for nuanced answer generation with a powerful API model for complex reasoning. By using a fine-tuned model, the assistant delivers responses with a consistent, expert tone, running efficiently on local hardware.

## üñ•Ô∏è User Interface

The Financial Analyst Assistant features a clean, intuitive Streamlit interface designed for seamless interaction with complex financial data:

![Financial Analyst Assistant Interface](Financial%20Analyst%20Assistant%20App.png)

The interface provides:
- **Real-time Chat**: Conversational interface for natural language queries about financial data
- **Context-Aware Responses**: The system maintains conversation history for follow-up questions
- **Professional Formatting**: Clean, readable responses with proper financial terminology
- **Responsive Design**: Optimized for both desktop and mobile viewing
- **Session Management**: Persistent conversation history throughout your analysis session

## ‚ú® Key Features

-   **Hybrid LLM Architecture**: Leverages the strengths of a specialized local model (Llama 3) for generation and a powerful API model (Gemini) for query analysis.
-   **Fine-Tuned for Finance**: Utilizes a `Meta-Llama-3-8B-Instruct` model fine-tuned on financial Q&A, providing expert, context-aware responses.
-   **High-Performance Local Inference**: Optimized with **Unsloth** for 2x faster, low-memory 4-bit inference on consumer GPUs.
-   **Hybrid Retrieval System**: Combines ChromaDB vector search with BM25 keyword search for robust document retrieval.
-   **Intelligent Query Construction**: Refines user queries using conversation context.
-   **Smart Caching**: Redis-based semantic similarity caching to reduce latency and redundant processing.
-   **Conversation Memory**: Persists conversation history using a PostgreSQL checkpointer.
-   **Interactive UI**: Built with Streamlit for a clean, real-time user experience.

## üèóÔ∏è Architecture

### The Hybrid LLM Strategy

This project employs two different LLMs, each assigned to the task it performs best:

1.  **Google Gemini Pro (Query Analysis)**: Used for its superior reasoning and structured output capabilities. It deconstructs user intent, handles conversation history to resolve follow-up questions, and extracts metadata filters for the retrieval system.
2.  **Fine-Tuned Llama 3 (Answer Generation)**: Used for the final, context-grounded response. As a specialized model, it delivers answers in the precise tone and format it was trained on, running efficiently and privately on local hardware.

### Graph Architecture

![Financial Analyst Assistant Graph](Financial%20Analyst%20Assistant%20Graph.png)

The system follows a sophisticated LangGraph workflow:
1.  **Query Construction**: Gemini analyzes user intent and conversation history.
2.  **Cache Check**: Searches Redis cache for similar previous queries.
3.  **Hybrid Retrieval**: If cache miss, performs semantic + keyword search.
4.  **Reranking**: A CrossEncoder model ranks and selects the top 2 most relevant documents.
5.  **Answer Generation**: The fine-tuned Llama 3 model generates the final response, which is then stored in the cache.

## üõ†Ô∏è Technical Stack

-   **Answer Generation LLM**: Fine-tuned `Meta-Llama-3-8B-Instruct`
-   **Query Analysis LLM**: Google Gemini 1.5 Pro
-   **Fine-Tuning & Inference Engine**: Unsloth
-   **ML Framework**: PyTorch
-   **Embeddings**: `BAAI/bge-small-en-v1.5` (FastEmbed)
-   **Vector Database**: ChromaDB
-   **Keyword Search**: `rank_bm25`
-   **Caching**: Redis Vector Store
-   **Reranking**: `cross-encoder/ms-marco-MiniLM-L-6-v2`
-   **Orchestration**: LangGraph
-   **Persistence**: PostgreSQL
-   **Frontend**: Streamlit

## üìã Prerequisites

-   **Hardware**: An **NVIDIA GPU** with CUDA 12.1+ support.
    -   **Minimum 8GB VRAM** is required to run the application.
    -   12GB+ VRAM is recommended for a smoother experience.
-   **Python**: Python 3.11 (64-bit).
-   **Docker**: For running the PostgreSQL and Redis databases.
-   **API Keys**: A Google Gemini API key.

## üöÄ Installation & Setup

1.  **Clone the repository**
    ```bash
    git clone https://github.com/eslammohamedtolba/Financial-Insight-Engine.git
    cd Financial-Insight-Engine
    ```

2.  **Create and activate virtual environment**
    ```bash
    # Create the environment
    python -m venv venv

    # Activate on Windows
    .\venv\Scripts\activate

    # Activate on macOS/Linux
    source venv/bin/activate
    ```

3.  **Install Dependencies**
    The `requirements.txt` file is configured to install all packages, including the correct CUDA version of PyTorch, in a single step.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up environment variables**
    Create a `.env` file in the root directory and add your credentials:
    ```env
    # Google Gemini API
    GOOGLE_API_KEY="your_gemini_api_key"

    # Database Configuration
    DB_USER="myuser"
    DB_PASSWORD="mypassword"
    DB_HOST="localhost"
    DB_PORT="5432"
    DB_NAME="graph_memory"
    ```

5.  **Start Docker containers**
    Run the following commands to start the PostgreSQL and Redis services.

    **PostgreSQL:**
    ```bash
    docker run --name my-postgres -d -p 5432:5432 -e POSTGRES_USER=myuser -e POSTGRES_PASSWORD=mypassword -e POSTGRES_DB=graph_memory -v postgres_data:/var/lib/postgresql/data postgres:16
    ```
    **Redis:**
    ```bash
    docker run --name my-redis -d -p 6379:6379 -v redis-data:/data redis/redis-stack:latest
    ```

6.  **Prepare Data and Fine-Tune Model (CRITICAL STEP)**
    You must run the provided Jupyter Notebooks in order. This will download the financial data, create the necessary database indexes, and fine-tune the Llama 3 model.

    - **Prepare Data:**
    Open and run all cells in `Data/"Financial Data Analysis Preparation.ipynb"`. This will create the ChromaDB vector store and the BM25 retriever file.

    - **Fine-Tune the Model:**
    Open and run all cells in `Data/"Finetune_Llama3_Financial_Analysis.ipynb"`. This will create the `Data/complete_finetuned_model` directory, which is required by the application.

7.  **Launch the application**
    Ensure all setup steps are complete and your Docker containers are running.
    ```bash
    streamlit run main.py
    ```

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ Data/
‚îÇ   ‚îú‚îÄ‚îÄ chroma_db/                                  # (Generated) ChromaDB vector store
‚îÇ   ‚îú‚îÄ‚îÄ complete_finetuned_model/                   # (Generated) Fine-tuned Llama 3 model
‚îÇ   ‚îú‚îÄ‚îÄ bm25_retriever.pkl                          # (Generated) Serialized BM25 retriever
‚îÇ   ‚îú‚îÄ‚îÄ Financial Data Analysis Preparation.ipynb   # Notebook to create and save the vector & keyword databases
‚îÇ   ‚îî‚îÄ‚îÄ Finetune_Llama3_Financial_Analysis.ipynb    # Notebook to fine-tune and save the Llama 3 model
‚îú‚îÄ‚îÄ .env                                            # Local environment variables
‚îú‚îÄ‚îÄ .gitignore                                      # Git ignore file
‚îú‚îÄ‚îÄ config.py                                       # Configuration and model initialization
‚îú‚îÄ‚îÄ db_utils.py                                     # Database utility functions
‚îú‚îÄ‚îÄ graph.py                                        # LangGraph workflow definition
‚îú‚îÄ‚îÄ main.py                                         # Streamlit application entry point
‚îú‚îÄ‚îÄ nodes.py                                        # Individual processing nodes
‚îú‚îÄ‚îÄ README.md                                       # Project README file
‚îú‚îÄ‚îÄ requirements.txt                                # Python dependencies
‚îú‚îÄ‚îÄ state.py                                        # State management and Pydantic models
‚îú‚îÄ‚îÄ Financial Analyst Assistant App.png             # UI Screenshot
‚îî‚îÄ‚îÄ Financial Analyst Assistant Graph.png           # Graph Screenshot

```

## üîí Safety & Compliance

-   Relaxed safety settings on Gemini for financial document analysis.
-   Proper error handling and fallback mechanisms.
-   Secure environment variable management using `.env` files.

## ü§ù Contributing

1.  Fork the repository
2.  Create a feature branch (`git checkout -b feature/AmazingFeature`)
3.  Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push to the branch (`git push origin feature/AmazingFeature`)
5.  Open a Pull Request