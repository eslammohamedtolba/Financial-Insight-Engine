{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWiOu2lUif2Y"
      },
      "source": [
        "### Install Dependencies\n",
        "\n",
        "First, we'll set up our environment by installing the necessary Python libraries.\n",
        "\n",
        "* **`unsloth`**: We install the latest version of Unsloth directly from its GitHub repository. Unsloth provides massive speedups and memory reductions for fine-tuning LLMs, enabling us to train models up to 2x faster and use 60% less memory. The `[colab-new]` option ensures compatibility with the latest Google Colab environments.\n",
        "* **Hugging Face Ecosystem**: We install key libraries for training and optimization:\n",
        "    * `peft`: Parameter-Efficient Fine-Tuning, for using techniques like LoRA.\n",
        "    * `trl`: Transformer Reinforcement Learning, for its easy-to-use `SFTTrainer`.\n",
        "    * `accelerate`: To easily run our training script on any hardware.\n",
        "    * `bitsandbytes`: For 4-bit quantization (QLoRA), which drastically reduces model size.\n",
        "* **`xformers`**: Provides memory-efficient attention mechanisms for another performance boost.\n",
        "* **`wandb`**: Weights & Biases, for logging our experiments and tracking metrics like training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O87oYkYiif2d",
        "outputId": "83e23f6c-8d40-4168-f46c-9760a7627e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes psutil\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvQhvrmbif2g"
      },
      "source": [
        "### Log In to Services\n",
        "\n",
        "To download our model and log our training progress, we need to authenticate with Hugging Face and Weights & Biases (W&B). We use Colab's `userdata` to securely access our API keys without hardcoding them in the notebook.\n",
        "\n",
        "* **Hugging Face Hub**: We need to log in to download the Phi-3 model, which requires accepting user conditions.\n",
        "* **Weights & Biases**: We log in to `wandb` to enable experiment tracking. This will allow us to monitor metrics like training loss in real-time.\n",
        "\n",
        "> **Action Required:** Before running this cell, you must store your API keys as secrets in Google Colab.\n",
        "> 1.  Click the **üîë (Secrets)** icon on the left sidebar.\n",
        "> 2.  Create a new secret named `hugging` and paste your Hugging Face access token (with `write` permissions) as the value.\n",
        "> 3.  Create another secret named `WANDB_API_KEY` and paste your W&B API key as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBO_NYgif2j",
        "outputId": "ed126a21-313e-4773-a738-9867338f436b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meslam3012mohamed\u001b[0m (\u001b[33meslam3012mohamed-not-yet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Log in to huggingface\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('hugging')\n",
        "\n",
        "# Log in to wandb\n",
        "import wandb\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUXELTvgif2l"
      },
      "source": [
        "### Import Core Libraries\n",
        "\n",
        "With our environment set up and authenticated, we can now import the core components from the libraries we installed. Each of these plays a critical role in the fine-tuning pipeline.\n",
        "\n",
        "* **`FastLanguageModel`**: The star of the show from Unsloth. This class will load our base model and automatically apply all the necessary optimizations for fast, memory-efficient training.\n",
        "* **`torch`**: The fundamental deep learning framework.\n",
        "* **`load_dataset`**: A function from the Hugging Face `datasets` library to easily pull our training data from the Hub.\n",
        "* **`SFTTrainer`**: A specialized trainer from the `trl` library designed specifically for Supervised Fine-Tuning.\n",
        "* **`TrainingArguments`**: A configuration class from `transformers` where we will define all the hyperparameters for our training job.\n",
        "* **`is_bfloat16_supported`**: A utility from Unsloth to check if our hardware supports `bfloat16` precision, which is ideal for training modern transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov4Dmgvvif2m",
        "outputId": "ae2d283c-e956-49a5-c32d-e50440152a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DQTpChNif2n"
      },
      "source": [
        "### Load Model and Tokenizer\n",
        "\n",
        "Now we use Unsloth's `FastLanguageModel` to load our pre-trained model. This single, powerful command handles several critical steps for us:\n",
        "\n",
        "1.  **Downloads the model** from the Hugging Face Hub.\n",
        "2.  **Applies 4-bit quantization** to drastically reduce memory usage.\n",
        "3.  **Patches the model** with performance optimizations for faster training.\n",
        "4.  **Prepares the tokenizer** for use in training.\n",
        "\n",
        "Let's look at the key parameters:\n",
        "* `model_name`: We are loading `\"microsoft/Phi-3-mini-4k-instruct\"`, a highly capable small language model that is perfect for fine-tuning on consumer hardware.\n",
        "* `load_in_4bit = True`: This is the core of our memory-saving strategy. It enables 4-bit quantization (QLoRA), reducing the VRAM footprint significantly.\n",
        "* `max_seq_length = 2048`: We set the maximum context window for our training examples. This offers a good balance between capturing long-range dependencies and managing memory.\n",
        "* `dtype = None`: This allows Unsloth to automatically detect and use the optimal data type for our GPU (like `bfloat16`), ensuring the best possible training performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G7pVFthif2o",
        "outputId": "3aff0c3f-7837-4779-a585-b6d6cf85b932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.9: Fast Mistral patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        "    token = hf_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMj8Kjrlif2o"
      },
      "source": [
        "### Test the Base Model (Before Fine-Tuning)\n",
        "\n",
        "Before we fine-tune the model, it's crucial to establish a baseline. We need to see how the pre-trained model performs on a task similar to our goal. This helps us understand its out-of-the-box capabilities and gives us a \"before\" snapshot to compare against our \"after\" fine-tuned version.\n",
        "\n",
        "Our test process involves a few key steps:\n",
        "1.  **Craft a Prompt**: We create a sample conversation using the standard `system` and `user` roles. The system prompt sets the model's persona (an expert financial analyst), while the user prompt provides a specific context and a question.\n",
        "2.  **Apply Chat Template**: We use `tokenizer.apply_chat_template`. This is a critical function that formats our structured conversation into the exact string format that `Phi-3-instruct` expects, including special tokens.\n",
        "3.  **Generate Response**: We run a standard inference using `model.generate()` to get the model's answer based on our prompt.\n",
        "4.  **Evaluate Output**: We'll examine the response to see if the model correctly follows instructions and extracts the required information from the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iafKlWsgif2p"
      },
      "outputs": [],
      "source": [
        "# Test base model first to ensure it works\n",
        "def test_model(model, tokenizer, context, question):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert financial analyst. Answer the user's question based only on the provided context.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Context: {context}\\nQuestion:{question}\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Use the model's built-in chat template\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    print(\"Generated prompt:\")\n",
        "    print(prompt)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    response_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1YfhKdHNif2q"
      },
      "outputs": [],
      "source": [
        "# Test base model first\n",
        "context = \"The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\"\n",
        "question = \"What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_response = test_model(base_model, tokenizer, context, question)\n",
        "print(\"=\" * 50,\"\\nBase model response:\",base_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbcKWiFilkxF",
        "outputId": "a48857e8-b153-47b7-ab26-0e12e846059d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prompt:\n",
            "<|system|>\n",
            "You are an expert financial analyst. Answer the user's question based only on the provided context.<|end|>\n",
            "<|user|>\n",
            "Context: The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\n",
            "Question:What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "================================================== \n",
            "Base model response: The two key factors that contributed to the increase in the company's gross margin in fiscal year 2023 were a favorable product mix with higher sales of premium software subscriptions, and manufacturing efficiencies gained from the new automated production line in Alexandria, Egypt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ex0zkHIif2r"
      },
      "source": [
        "### Configure LoRA for Efficient Fine-Tuning\n",
        "\n",
        "Now we get to the core of Parameter-Efficient Fine-Tuning (PEFT). Instead of training the entire model, we'll use **Low-Rank Adaptation (LoRA)** to inject small, trainable \"adapter\" matrices into the model's architecture. This means we only need to train a tiny fraction of the total parameters (typically <1%), which is what makes fine-tuning feasible on a single GPU.\n",
        "\n",
        "Unsloth's `get_peft_model` function seamlessly applies this configuration to our 4-bit model. Let's look at the key hyperparameters:\n",
        "\n",
        "* `r = 16`: The rank or dimension of the LoRA adapter matrices. A higher rank means more trainable parameters and greater expressive power, but also more memory. `16` is a solid and popular choice.\n",
        "* `lora_alpha = 16`: The scaling factor for the LoRA weights. A common convention is to set this equal to `r`.\n",
        "* `target_modules`: This is a critical setting. We specify the names of the layers (in this case, the attention and feed-forward layers) where the LoRA adapters will be injected. Unsloth provides a utility to find all potential layers, and we're targeting the most impactful ones here.\n",
        "* `use_gradient_checkpointing = \"unsloth\"`: A crucial memory-saving technique that trades a bit of computation time to drastically reduce VRAM usage, allowing us to use larger batch sizes or longer sequences. The `\"unsloth\"` option enables a custom, faster implementation.\n",
        "* `random_state = 2002`: We set a seed for reproducibility. Fun fact: 2002 was the year the modern Bibliotheca Alexandrina was inaugurated, not too far from our model's fictional manufacturing plant in Alexandria.\n",
        "\n",
        "After this cell, our model is fully prepared for training. The original weights are frozen, and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFXZWtJ_if2r",
        "outputId": "e1e3ceca-82b6-4160-bdcb-eb1b6b54df1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.12.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "ft_model = FastLanguageModel.get_peft_model(\n",
        "    model = base_model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 2002,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Know the number of paarmeters and the percentage that will be unfreezed\n",
        "ft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubf2XBol6zUF",
        "outputId": "74e8c0f3-fc12-44cd-991c-78d8b407b58b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 29,884,416 || all params: 3,850,963,968 || trainable%: 0.7760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zq_lNjif2s"
      },
      "source": [
        "### Load and Prepare the Dataset\n",
        "\n",
        "The quality and format of your training data are paramount for a successful fine-tune. Instruction-tuned models like Phi-3 are highly sensitive to the prompt format they were trained on. In this step, we will load our financial Q&A dataset and transform each entry to perfectly match Phi-3's specific chat template.\n",
        "\n",
        "Our workflow is as follows:\n",
        "1.  **Load Dataset**: We start by loading the `virattt/llama-3-8b-financialQA` dataset from the Hugging Face Hub. This dataset contains pairs of financial contexts, questions, and expert answers.\n",
        "2.  **Define a Formatting Function**: We create a function, `formatting_prompts_func`, that takes a batch of examples and restructures them. For each row, it builds a conversation with three parts:\n",
        "    * A `system` message to consistently set the model's persona.\n",
        "    * A `user` message combining the `context` and `question`.\n",
        "    * An `assistant` message containing the ground-truth `answer` that we want the model to learn.\n",
        "3.  **Apply the Chat Template**: Inside the function, we use the crucial `tokenizer.apply_chat_template` method. This converts the structured conversation into"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o1BqEOZQif2s"
      },
      "outputs": [],
      "source": [
        "# Use Phi-3's actual chat template for training\n",
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    contexts = examples[\"context\"]\n",
        "    responses = examples[\"answer\"]\n",
        "    texts = []\n",
        "\n",
        "    for question, context, response in zip(questions, contexts, responses):\n",
        "        # Create proper conversation format\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an expert financial analyst. Answer the user's question based only on the provided context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": response\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Use the model's chat template\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)\n",
        "\n",
        "    return {\"text\": texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJpHbTsTif2t",
        "outputId": "b3088aa6-881b-412a-e52e-4eb6c6963ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample before formatting: {'question': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?', 'answer': 'NVIDIA initially focused on PC graphics.', 'context': 'Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.', 'ticker': 'NVDA', 'filing': '2023_10K'}\n"
          ]
        }
      ],
      "source": [
        "# Load and format dataset\n",
        "dataset = load_dataset(\"virattt/llama-3-8b-financialQA\", split=\"train\")\n",
        "\n",
        "print(\"Sample before formatting:\", dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show dataset column names\n",
        "dataset.column_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHBbbfcSwU8K",
        "outputId": "3363c952-cc2c-4190-a5de-5a639cba3237"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['question', 'answer', 'context', 'ticker', 'filing', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktdD9JD8if2u",
        "outputId": "107219d0-4c7a-4608-840f-1dd193b1bc7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Sample after formatting:',\n",
              " \"<|system|>\\nYou are an expert financial analyst. Answer the user's question based only on the provided context.<|end|>\\n<|user|>\\nContext: Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.\\n\\nQuestion: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?<|end|>\\n<|assistant|>\\nNVIDIA initially focused on PC graphics.<|end|>\\n<|endoftext|>\",\n",
              " '...')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "\"Sample after formatting:\", dataset[0][\"text\"][:500], \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNMy3exuif2u"
      },
      "source": [
        "### Configure and Launch the Fine-Tuning Job\n",
        "\n",
        "We have arrived at the final step. With our model loaded, LoRA configured, and the dataset perfectly formatted, we can now set up the trainer and launch the fine-tuning process.\n",
        "\n",
        "We will use the `SFTTrainer` from the TRL library, which handles the complexities of the training loop for us. The behavior of the trainer is controlled by a comprehensive set of `TrainingArguments`.\n",
        "\n",
        "#### Key Hyperparameters:\n",
        "* **Batching**: We use a `per_device_train_batch_size` of 2 and `gradient_accumulation_steps` of 4. This gives us an effective batch size of `2 * 4 = 8`, which helps stabilize training while keeping memory usage low.\n",
        "* **Training Steps**: We set `max_steps = 60` for a short, demonstrative training run. In a real-world scenario, you would train for more steps or for a certain number of epochs.\n",
        "* **Learning Rate**: A `learning_rate` of `2e-4` with a linear scheduler and a few `warmup_steps` is a standard and effective setup for LoRA.\n",
        "* **Optimizations**: We use the `adamw_8bit` optimizer and enable `bf16` (bfloat16 mixed-precision) if our GPU supports it. These are powerful techniques that accelerate training and reduce memory consumption.\n",
        "* **Logging and Saving**: We `logging_steps = 1` to see the loss at every step and will save a model checkpoint to the `outputs` directory halfway through training (`save_steps = 30`).\n",
        "\n",
        "#### Launching the Training\n",
        "With all the components in place, a single call to `trainer.train()` starts the fine-tuning process. As the training commences, keep an eye on the logged training loss‚Äîit should steadily decrease, indicating that the model is learning from our financial Q&A data. Let's kick it off and let the GPU work its magic through the early hours of this Saturday morning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    save_strategy = \"steps\",\n",
        "    warmup_steps = 5,\n",
        "    num_train_epochs = 5,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    optim = \"adamw_8bit\",\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    output_dir = \"outputs\",\n",
        "    logging_steps = 100,\n",
        "    seed = 2002,\n",
        "    max_steps = 60,\n",
        "    weight_decay = 0.01,\n",
        "    save_steps = 30,\n",
        ")"
      ],
      "metadata": {
        "id": "9ggaEX1Fn0Qc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wlH2aAM6if2v"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "trainer = SFTTrainer(\n",
        "    model = ft_model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False,\n",
        "    args = training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "5hDuHStKif2w",
        "outputId": "68743929-5e95-40f8-de14-8c06bed8ba92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 7,000 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 05:21, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>4303114843299840.0</td></tr><tr><td>train/epoch</td><td>0.13714</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train_loss</td><td>1.05234</td></tr><tr><td>train_runtime</td><td>327.2726</td></tr><tr><td>train_samples_per_second</td><td>2.933</td></tr><tr><td>train_steps_per_second</td><td>0.183</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worldly-dew-1</strong> at: <a href='https://wandb.ai/eslam3012mohamed-not-yet/huggingface/runs/c3wkn8h8' target=\"_blank\">https://wandb.ai/eslam3012mohamed-not-yet/huggingface/runs/c3wkn8h8</a><br> View project at: <a href='https://wandb.ai/eslam3012mohamed-not-yet/huggingface' target=\"_blank\">https://wandb.ai/eslam3012mohamed-not-yet/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251224_092829-c3wkn8h8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtidVsUzif2x"
      },
      "source": [
        "### Test the Fine-Tuned Model\n",
        "\n",
        "The training is complete! Now for the moment of truth: did our fine-tuning work? We will now test our specialized model and compare its performance directly against the baseline we established in Step 5.\n",
        "\n",
        "To ensure a fair evaluation, our process is simple but critical:\n",
        "1.  **Use a Consistent Prompt Format**: Our new `inference` function formats the prompt using the **exact same** system message and chat template that the model was trained on. This consistency is crucial for unlocking the model's new capabilities.\n",
        "2.  **Rerun the Original Test Case**: We will ask the **exact same question** using the same context from our baseline test.\n",
        "\n",
        "This provides our \"after\" snapshot. Compare this response to the one from the base model. Look for improvements in accuracy, conciseness, formatting (e.g., using a proper list), and overall adherence to the system prompt's instructions.\n",
        "\n",
        "After a short but intense training session in the quiet of the Giza night, let's see how our newly specialized financial analyst performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrhasPKVif2z",
        "outputId": "23e71a53-6e2c-429c-aa10-50e9b4055f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated prompt:\n",
            "<|system|>\n",
            "You are an expert financial analyst. Answer the user's question based only on the provided context.<|end|>\n",
            "<|user|>\n",
            "Context: The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\n",
            "Question:What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "\n",
            "==================================================\n",
            "Question: What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\n",
            "Response: The gross margin increased due to a favorable product mix with higher sales of premium software subscriptions and manufacturing efficiencies gained from a new automated production line in Alexandria, Egypt.\n"
          ]
        }
      ],
      "source": [
        "# Test the fine-tuned model\n",
        "context = \"The company's gross margin improved to 45.8% in fiscal year 2023, up from 42.1% in the prior year. The margin expansion was mainly attributable to a favorable product mix with higher sales of our premium software subscriptions, and manufacturing efficiencies gained from our new automated production line in Alexandria, Egypt.\"\n",
        "question = \"What were the two key factors that contributed to the increase in the company's gross margin in fiscal year 2023?\"\n",
        "\n",
        "response = test_model(ft_model, tokenizer, context, question)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "E4r4hH7NA76L"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "QhjMTr_ovB9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q evaluate sentence-transformers bert-score scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkE01_Zgvm_c",
        "outputId": "3e7bbd44-6067-471e-a51b-f2b423217be4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ProductionModelEvaluator:\n",
        "  \"\"\"\n",
        "  Comprehensive evaluation framework for production-ready language models.\n",
        "  Measures latency, throughput, quality metrics, and resource usage.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model, tokenizer, max_new_tokens=20, device='cuda'):\n",
        "      \"\"\"\n",
        "      Initialize evaluator with model and tokenizer.\n",
        "\n",
        "      Args:\n",
        "          model: Finetuned language model\n",
        "          tokenizer: Corresponding tokenizer\n",
        "          device: 'cuda' or 'cpu'\n",
        "      \"\"\"\n",
        "      self.model = model\n",
        "      self.tokenizer = tokenizer\n",
        "      self.device = device\n",
        "      self.model.to(device)\n",
        "      self.max_new_tokens = max_new_tokens\n",
        "      self.model.eval()  # Set to evaluation mode\n",
        "\n",
        "      # Storage for results\n",
        "      self.results = {\n",
        "          'latency': [],\n",
        "          'throughput': [],\n",
        "          'quality': {},\n",
        "          'resource': {}\n",
        "      }\n",
        "\n",
        "  # ============================================================\n",
        "  # 1. LATENCY METRICS - How fast is inference?\n",
        "  # ============================================================\n",
        "\n",
        "  def measure_latency(self, test_inputs, num_runs=100, warmup_runs=10):\n",
        "    \"\"\"\n",
        "    Measure inference latency metrics including tokenization and decoding.\n",
        "\n",
        "    Args:\n",
        "        test_inputs: List of input texts to test\n",
        "        num_runs: Number of runs for averaging\n",
        "        warmup_runs: GPU warmup iterations\n",
        "\n",
        "    Returns:\n",
        "        dict with latency statistics\n",
        "    \"\"\"\n",
        "    print(\"üìä Measuring Latency...\")\n",
        "    total_latencies = []\n",
        "    ttft_latencies = []\n",
        "    tgt_latencies = []\n",
        "    tpot_latencies = []\n",
        "\n",
        "    # Warmup GPU\n",
        "    for _ in range(warmup_runs):\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(test_inputs[0], return_tensors='pt').to(self.device)\n",
        "            _ = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens)\n",
        "\n",
        "    # Measure actual latency\n",
        "    for _ in tqdm(range(num_runs), desc=\"Latency Test\"):\n",
        "        test_text = np.random.choice(test_inputs)\n",
        "\n",
        "        # Start timing (includes tokenization)\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(test_text, return_tensors='pt').to(self.device)\n",
        "            input_length = inputs['input_ids'].shape[1]  # Get input length BEFORE generation\n",
        "\n",
        "            # Generate\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_new_tokens,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "            # Extract ONLY the new tokens (not the input prompt)\n",
        "            generated_ids = outputs.sequences[0]\n",
        "            new_tokens_ids = generated_ids[input_length:]  # Slice to get only new tokens\n",
        "\n",
        "            # Decode only the NEW tokens\n",
        "            output_text = self.tokenizer.decode(new_tokens_ids, skip_special_tokens=True)\n",
        "\n",
        "            # End timing (includes decoding)\n",
        "            end_time = time.time()\n",
        "\n",
        "        # Do calculations outside timing\n",
        "        output_length = generated_ids.shape[0]\n",
        "        num_new_tokens = output_length - input_length\n",
        "\n",
        "        # Total latency (entire process)\n",
        "        total_latency = (end_time - start_time) * 1000  # ms\n",
        "        total_latencies.append(total_latency)\n",
        "\n",
        "        # TTFT approximation\n",
        "        # For autoregressive models, first token typically takes 10-15% of total time\n",
        "        ttft = total_latency * 0.12  # More realistic approximation\n",
        "        ttft_latencies.append(ttft)\n",
        "\n",
        "        # TGT: Time after first token\n",
        "        tgt = total_latency - ttft\n",
        "        tgt_latencies.append(tgt)\n",
        "\n",
        "        # TPOT: Time per output token\n",
        "        if num_new_tokens > 1:\n",
        "            tpot = tgt / (num_new_tokens - 1)\n",
        "        else:\n",
        "            tpot = tgt\n",
        "        tpot_latencies.append(tpot)\n",
        "\n",
        "    latency_stats = {\n",
        "        'total_mean_ms': np.mean(total_latencies),\n",
        "        'total_median_ms': np.median(total_latencies),\n",
        "        'total_p95_ms': np.percentile(total_latencies, 95),\n",
        "        'total_p99_ms': np.percentile(total_latencies, 99),\n",
        "        'std_ms': np.std(total_latencies),\n",
        "        'min_ms': np.min(total_latencies),\n",
        "        'max_ms': np.max(total_latencies),\n",
        "        'ttft_mean_ms': np.mean(ttft_latencies),\n",
        "        'ttft_median_ms': np.median(ttft_latencies),\n",
        "        'ttft_p95_ms': np.percentile(ttft_latencies, 95),\n",
        "        'tgt_mean_ms': np.mean(tgt_latencies),\n",
        "        'tgt_median_ms': np.median(tgt_latencies),\n",
        "        'tpot_mean_ms': np.mean(tpot_latencies),\n",
        "        'tpot_median_ms': np.median(tpot_latencies),\n",
        "        'tpot_p95_ms': np.percentile(tpot_latencies, 95)\n",
        "    }\n",
        "\n",
        "    self.results['latency'] = latency_stats\n",
        "    self._print_latency_results(latency_stats)\n",
        "    return latency_stats\n",
        "\n",
        "  def _print_latency_results(self, stats):\n",
        "      \"\"\"Print latency statistics in readable format.\"\"\"\n",
        "      print(\"\\n‚úÖ Latency Results:\")\n",
        "      print(f\"\\n  Total Latency:\")\n",
        "      print(f\"    Mean: {stats['total_mean_ms']:.2f} ms\")\n",
        "      print(f\"    P95: {stats['total_p95_ms']:.2f} ms\")\n",
        "\n",
        "      print(f\"\\n  TTFT (Time to First Token):\")\n",
        "      print(f\"    Mean: {stats['ttft_mean_ms']:.2f} ms\")\n",
        "      print(f\"    P95: {stats['ttft_p95_ms']:.2f} ms\")\n",
        "\n",
        "      print(f\"\\n  TGT (Token Generation Time):\")\n",
        "      print(f\"    Mean: {stats['tgt_mean_ms']:.2f} ms\")\n",
        "\n",
        "      print(f\"\\n  TPOT (Time Per Output Token):\")\n",
        "      print(f\"    Mean: {stats['tpot_mean_ms']:.2f} ms\")\n",
        "      print(f\"    P95: {stats['tpot_p95_ms']:.2f} ms\")\n",
        "\n",
        "      # Production readiness\n",
        "      if stats['total_p95_ms'] < 100:\n",
        "          print(\"\\n  ‚úÖ EXCELLENT - Production ready\")\n",
        "      elif stats['total_p95_ms'] < 500:\n",
        "          print(\"\\n  ‚ö†Ô∏è  GOOD - Acceptable\")\n",
        "      else:\n",
        "          print(\"\\n  ‚ùå SLOW - Needs optimization\")\n",
        "\n",
        "  # ============================================================\n",
        "  # 2. THROUGHPUT METRICS - How many requests per second?\n",
        "  # ============================================================\n",
        "\n",
        "  def measure_throughput(self, test_inputs, batch_sizes=[1, 4, 8, 16]):\n",
        "    \"\"\"\n",
        "    Measure throughput (requests/second and tokens/second) for different batch sizes.\n",
        "    Includes tokenization and decoding in timing.\n",
        "\n",
        "    Args:\n",
        "        test_inputs: List of input texts\n",
        "        batch_sizes: List of batch sizes to test\n",
        "\n",
        "    Returns:\n",
        "        dict with throughput for each batch size\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Measuring Throughput...\")\n",
        "    throughput_results = {}\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        batch_texts = test_inputs[:batch_size] * 10\n",
        "\n",
        "        total_processed = 0\n",
        "        total_tokens_generated = 0\n",
        "        total_elapsed = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(batch_texts), batch_size):\n",
        "                batch = batch_texts[i:i+batch_size]\n",
        "\n",
        "                # Start timing for this batch\n",
        "                batch_start = time.time()\n",
        "\n",
        "                # Tokenize\n",
        "                inputs = self.tokenizer(\n",
        "                    batch,\n",
        "                    return_tensors='pt',\n",
        "                    padding=True,\n",
        "                    truncation=True\n",
        "                ).to(self.device)\n",
        "\n",
        "                input_length = inputs['input_ids'].shape[1]  # Get input length\n",
        "\n",
        "                # Generate\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=self.max_new_tokens\n",
        "                )\n",
        "\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                # Extract only NEW tokens for each sequence in batch\n",
        "                new_tokens_batch = outputs[:, input_length:]  # Slice all sequences\n",
        "\n",
        "                # Decode (part of user-facing process)\n",
        "                decoded_outputs = [\n",
        "                    self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "                    for new_tokens in new_tokens_batch  # Decode only new tokens\n",
        "                ]\n",
        "\n",
        "                # End timing for this batch\n",
        "                batch_end = time.time()\n",
        "\n",
        "                # Add to total elapsed time\n",
        "                total_elapsed += (batch_end - batch_start)\n",
        "\n",
        "                # Do calculations\n",
        "                total_processed += len(batch)\n",
        "                # Count only NEW tokens generated\n",
        "                tokens_per_sequence = new_tokens_batch.shape[1]  # Shape of new tokens only\n",
        "                total_tokens_generated += tokens_per_sequence * len(batch)\n",
        "\n",
        "        # Calculate metrics using summed elapsed time\n",
        "        rps = total_processed / total_elapsed\n",
        "        tps = total_tokens_generated / total_elapsed\n",
        "\n",
        "        throughput_results[batch_size] = {'rps': rps, 'tps': tps}\n",
        "\n",
        "        print(f\"  Batch size {batch_size}:\")\n",
        "        print(f\"    RPS (Requests/sec): {rps:.2f}\")\n",
        "        print(f\"    TPS (Tokens/sec): {tps:.2f}\")\n",
        "\n",
        "    self.results['throughput'] = throughput_results\n",
        "    return throughput_results\n",
        "\n",
        "  # ============================================================\n",
        "  # 3. QUALITY METRICS - How good are the predictions?\n",
        "  # ============================================================\n",
        "\n",
        "  def evaluate_quality(self, test_data, metrics=['bleu', 'rouge', 'cosine', 'bertscore', 'bi_encoder']):\n",
        "    \"\"\"\n",
        "    Evaluate generation quality using standard NLP metrics.\n",
        "\n",
        "    Args:\n",
        "        test_data: List of dicts with 'input' and 'expected_output'\n",
        "        metrics: List of metrics to compute\n",
        "\n",
        "    Returns:\n",
        "        dict with quality scores\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Evaluating Quality...\")\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Generate predictions\n",
        "    for sample in tqdm(test_data, desc=\"Generating predictions\"):\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(sample['input'], return_tensors='pt').to(self.device)\n",
        "            input_length = inputs['input_ids'].shape[1]  # Get input length\n",
        "\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=self.max_new_tokens\n",
        "            )\n",
        "\n",
        "            # Extract only NEW tokens (not the input prompt)\n",
        "            new_tokens = outputs[0][input_length:]  # Slice to get only new tokens\n",
        "            pred_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "            predictions.append(pred_text)\n",
        "            references.append(sample['expected_output'])\n",
        "\n",
        "    quality_scores = {}\n",
        "\n",
        "    # Calculate BLEU score (measures n-gram overlap)\n",
        "    if 'bleu' in metrics:\n",
        "        bleu_scores = []\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            pred_words = set(pred.lower().split())\n",
        "            ref_words = set(ref.lower().split())\n",
        "            if len(ref_words) > 0:\n",
        "                overlap = len(pred_words.intersection(ref_words))\n",
        "                bleu = overlap / len(ref_words)\n",
        "                bleu_scores.append(bleu)\n",
        "\n",
        "        quality_scores['bleu'] = np.mean(bleu_scores)\n",
        "        print(f\"  BLEU Score: {quality_scores['bleu']:.4f}\")\n",
        "\n",
        "    # Cosine Similarity with TF-IDF\n",
        "    if 'cosine' in metrics:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "        print(\"  Computing Cosine Similarity...\")\n",
        "        cosine_scores = []\n",
        "\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            try:\n",
        "                vectorizer = TfidfVectorizer()\n",
        "                tfidf_matrix = vectorizer.fit_transform([pred, ref])\n",
        "                cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "                cosine_scores.append(cosine_sim)\n",
        "            except:\n",
        "                cosine_scores.append(0.0)\n",
        "\n",
        "        quality_scores['cosine_similarity'] = np.mean(cosine_scores)\n",
        "        print(f\"  Cosine Similarity: {quality_scores['cosine_similarity']:.4f}\")\n",
        "\n",
        "    # BERTScore\n",
        "    if 'bertscore' in metrics:\n",
        "        from bert_score import score\n",
        "\n",
        "        print(\"  Computing BERTScore...\")\n",
        "        P, R, F1 = score(predictions, references, lang='en', verbose=False)\n",
        "\n",
        "        quality_scores['bertscore_precision'] = P.mean().item()\n",
        "        quality_scores['bertscore_recall'] = R.mean().item()\n",
        "        quality_scores['bertscore_f1'] = F1.mean().item()\n",
        "\n",
        "        print(f\"  BERTScore F1: {quality_scores['bertscore_f1']:.4f}\")\n",
        "        print(f\"  BERTScore Precision: {quality_scores['bertscore_precision']:.4f}\")\n",
        "        print(f\"  BERTScore Recall: {quality_scores['bertscore_recall']:.4f}\")\n",
        "\n",
        "    # Bi-Encoder Semantic Similarity\n",
        "    if 'bi_encoder' in metrics:\n",
        "        from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "        print(\"  Computing Bi-Encoder Similarity...\")\n",
        "\n",
        "        # Load bi-encoder model\n",
        "        bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Encode all predictions and references\n",
        "        pred_embeddings = bi_encoder.encode(predictions, convert_to_tensor=True)\n",
        "        ref_embeddings = bi_encoder.encode(references, convert_to_tensor=True)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        bi_encoder_scores = []\n",
        "        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):\n",
        "            similarity = util.cos_sim(pred_emb, ref_emb).item()\n",
        "            bi_encoder_scores.append(similarity)\n",
        "\n",
        "        quality_scores['bi_encoder_similarity'] = np.mean(bi_encoder_scores)\n",
        "        print(f\"  Bi-Encoder Similarity: {quality_scores['bi_encoder_similarity']:.4f}\")\n",
        "\n",
        "    # Calculate exact match accuracy\n",
        "    exact_matches = sum([1 for p, r in zip(predictions, references)\n",
        "                        if p.strip().lower() == r.strip().lower()])\n",
        "    quality_scores['exact_match'] = exact_matches / len(predictions)\n",
        "    print(f\"  Exact Match: {quality_scores['exact_match']:.2%}\")\n",
        "\n",
        "    # Calculate partial match\n",
        "    partial_matches = sum([1 for p, r in zip(predictions, references)\n",
        "                          if r.lower() in p.lower()])\n",
        "    quality_scores['partial_match'] = partial_matches / len(predictions)\n",
        "    print(f\"  Partial Match: {quality_scores['partial_match']:.2%}\")\n",
        "\n",
        "    # Store sample predictions\n",
        "    quality_scores['samples'] = [\n",
        "        {'input': test_data[i]['input'],\n",
        "         'expected': test_data[i]['expected_output'],\n",
        "         'predicted': predictions[i]}\n",
        "        for i in range(min(5, len(predictions)))\n",
        "    ]\n",
        "\n",
        "    self.results['quality'] = quality_scores\n",
        "    return quality_scores\n",
        "\n",
        "  # ============================================================\n",
        "  # 4. RESOURCE USAGE - Memory and GPU utilization\n",
        "  # ============================================================\n",
        "\n",
        "  def measure_resource_usage(self, test_input):\n",
        "    \"\"\"\n",
        "    Measure GPU memory usage during inference.\n",
        "\n",
        "    Args:\n",
        "        test_input: Single input text for testing\n",
        "\n",
        "    Returns:\n",
        "        dict with memory statistics\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Measuring Resource Usage...\")\n",
        "\n",
        "    # Reset memory stats and clear cache PROPERLY\n",
        "    if self.device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()  # Ensure all operations complete\n",
        "\n",
        "    # Measure memory after cache clear\n",
        "    if self.device == 'cuda':\n",
        "        mem_before = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        inputs = self.tokenizer(test_input, return_tensors='pt').to(self.device)\n",
        "        input_length = inputs['input_ids'].shape[1]  # Get input length\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=self.max_new_tokens\n",
        "        )\n",
        "\n",
        "        torch.cuda.synchronize()  # Ensure generation completes\n",
        "\n",
        "        # Extract only new tokens\n",
        "        new_tokens = outputs[0][input_length:]\n",
        "        decoded = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Measure memory after generation completes\n",
        "    if self.device == 'cuda':\n",
        "        torch.cuda.synchronize()  # Ensure all operations complete\n",
        "        mem_after = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "        mem_peak = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "\n",
        "        resource_stats = {\n",
        "            'memory_before_mb': mem_before,\n",
        "            'memory_after_mb': mem_after,\n",
        "            'memory_peak_mb': mem_peak,\n",
        "            'memory_used_mb': mem_peak - mem_before  # Use peak\n",
        "        }\n",
        "\n",
        "        print(f\"  Memory Before: {mem_before:.2f} MB\")\n",
        "        print(f\"  Memory After: {mem_after:.2f} MB\")\n",
        "        print(f\"  Memory Peak: {mem_peak:.2f} MB\")\n",
        "        print(f\"  Memory Used (Peak - Before): {resource_stats['memory_used_mb']:.2f} MB\")\n",
        "    else:\n",
        "        resource_stats = {'device': 'cpu', 'memory_tracking': 'not_available'}\n",
        "\n",
        "    self.results['resource'] = resource_stats\n",
        "    return resource_stats\n",
        "\n",
        "  # ============================================================\n",
        "  # 5. COMPREHENSIVE REPORT\n",
        "  # ============================================================\n",
        "\n",
        "  def generate_report(self, save_path='model_evaluation_report.html'):\n",
        "    \"\"\"\n",
        "    Generate comprehensive HTML report with all metrics.\n",
        "\n",
        "    Args:\n",
        "        save_path: Path to save HTML report\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìù Generating comprehensive report...\")\n",
        "\n",
        "    # Create report content\n",
        "    report = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Model Evaluation Report</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
        "            h1 {{ color: #333; }}\n",
        "            h2 {{ color: #666; border-bottom: 2px solid #ddd; padding-bottom: 10px; }}\n",
        "            .metric {{ background: #f5f5f5; padding: 15px; margin: 10px 0; border-radius: 5px; }}\n",
        "            .good {{ color: green; font-weight: bold; }}\n",
        "            .warning {{ color: orange; font-weight: bold; }}\n",
        "            .bad {{ color: red; font-weight: bold; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
        "            th {{ background-color: #4CAF50; color: white; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>üöÄ Production Model Evaluation Report</h1>\n",
        "        <p>Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "\n",
        "        <h2>‚ö° Latency Metrics</h2>\n",
        "        <div class=\"metric\">\n",
        "            <h3>Total Latency</h3>\n",
        "            <table>\n",
        "                <tr><th>Metric</th><th>Value</th></tr>\n",
        "                <tr><td>Mean</td><td>{self.results['latency']['total_mean_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Median</td><td>{self.results['latency']['total_median_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>P95</td><td>{self.results['latency']['total_p95_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>P99</td><td>{self.results['latency']['total_p99_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Standard Deviation</td><td>{self.results['latency']['std_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Min</td><td>{self.results['latency']['min_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Max</td><td>{self.results['latency']['max_ms']:.2f} ms</td></tr>\n",
        "            </table>\n",
        "\n",
        "            <h3>TTFT (Time to First Token)</h3>\n",
        "            <table>\n",
        "                <tr><th>Metric</th><th>Value</th></tr>\n",
        "                <tr><td>Mean</td><td>{self.results['latency']['ttft_mean_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Median</td><td>{self.results['latency']['ttft_median_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>P95</td><td>{self.results['latency']['ttft_p95_ms']:.2f} ms</td></tr>\n",
        "            </table>\n",
        "\n",
        "            <h3>TGT (Token Generation Time)</h3>\n",
        "            <table>\n",
        "                <tr><th>Metric</th><th>Value</th></tr>\n",
        "                <tr><td>Mean</td><td>{self.results['latency']['tgt_mean_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Median</td><td>{self.results['latency']['tgt_median_ms']:.2f} ms</td></tr>\n",
        "            </table>\n",
        "\n",
        "            <h3>TPOT (Time Per Output Token)</h3>\n",
        "            <table>\n",
        "                <tr><th>Metric</th><th>Value</th></tr>\n",
        "                <tr><td>Mean</td><td>{self.results['latency']['tpot_mean_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>Median</td><td>{self.results['latency']['tpot_median_ms']:.2f} ms</td></tr>\n",
        "                <tr><td>P95</td><td>{self.results['latency']['tpot_p95_ms']:.2f} ms</td></tr>\n",
        "            </table>\n",
        "        </div>\n",
        "\n",
        "        <h2>üìà Throughput</h2>\n",
        "        <div class=\"metric\">\n",
        "            <table>\n",
        "                <tr><th>Batch Size</th><th>Requests/Second</th><th>Tokens/Second</th></tr>\n",
        "    \"\"\"\n",
        "\n",
        "    for batch_size, throughput in self.results['throughput'].items():\n",
        "        report += f\"<tr><td>{batch_size}</td><td>{throughput['rps']:.2f}</td><td>{throughput['tps']:.2f}</td></tr>\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "            </table>\n",
        "        </div>\n",
        "\n",
        "        <h2>‚ú® Quality Metrics</h2>\n",
        "        <div class=\"metric\">\n",
        "            <table>\n",
        "                <tr><th>Metric</th><th>Score</th></tr>\n",
        "                <tr><td>BLEU Score</td><td>{self.results['quality'].get('bleu', 0):.4f}</td></tr>\n",
        "                <tr><td>Cosine Similarity</td><td>{self.results['quality'].get('cosine_similarity', 0):.4f}</td></tr>\n",
        "                <tr><td>BERTScore F1</td><td>{self.results['quality'].get('bertscore_f1', 0):.4f}</td></tr>\n",
        "                <tr><td>BERTScore Precision</td><td>{self.results['quality'].get('bertscore_precision', 0):.4f}</td></tr>\n",
        "                <tr><td>BERTScore Recall</td><td>{self.results['quality'].get('bertscore_recall', 0):.4f}</td></tr>\n",
        "                <tr><td>Bi-Encoder Similarity</td><td>{self.results['quality'].get('bi_encoder_similarity', 0):.4f}</td></tr>\n",
        "                <tr><td>Exact Match</td><td>{self.results['quality'].get('exact_match', 0):.2%}</td></tr>\n",
        "                <tr><td>Partial Match</td><td>{self.results['quality'].get('partial_match', 0):.2%}</td></tr>\n",
        "            </table>\n",
        "        </div>\n",
        "\n",
        "        <h2>üíæ Resource Usage</h2>\n",
        "        <div class=\"metric\">\n",
        "            <table>\n",
        "                <tr><th>Metric</th><th>Value</th></tr>\n",
        "                <tr><td>Memory Before</td><td>{self.results['resource'].get('memory_before_mb', 0):.2f} MB</td></tr>\n",
        "                <tr><td>Peak Memory</td><td>{self.results['resource'].get('memory_peak_mb', 0):.2f} MB</td></tr>\n",
        "                <tr><td>Memory Used</td><td>{self.results['resource'].get('memory_used_mb', 0):.2f} MB</td></tr>\n",
        "            </table>\n",
        "        </div>\n",
        "\n",
        "        <h2>üìã Sample Predictions</h2>\n",
        "    \"\"\"\n",
        "\n",
        "    if 'samples' in self.results['quality']:\n",
        "        for i, sample in enumerate(self.results['quality']['samples']):\n",
        "            # Escape HTML characters\n",
        "            input_text = sample['input'][:200].replace('<', '&lt;').replace('>', '&gt;')\n",
        "            expected_text = sample['expected'].replace('<', '&lt;').replace('>', '&gt;')\n",
        "            predicted_text = sample['predicted'].replace('<', '&lt;').replace('>', '&gt;')\n",
        "\n",
        "            report += f\"\"\"\n",
        "            <div class=\"metric\">\n",
        "                <p><strong>Sample {i+1}:</strong></p>\n",
        "                <p><strong>Input:</strong> {input_text}...</p>\n",
        "                <p><strong>Expected:</strong> {expected_text}</p>\n",
        "                <p><strong>Predicted:</strong> {predicted_text}</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "    report += \"\"\"\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Save report\n",
        "    with open(save_path, 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    print(f\"‚úÖ Report saved to: {save_path}\")\n",
        "\n",
        "  # ============================================================\n",
        "  # 6. PRODUCTION READINESS CHECK\n",
        "  # ============================================================\n",
        "\n",
        "  def production_readiness_check(self):\n",
        "    \"\"\"\n",
        "    Determine if model is production-ready based on all metrics.\n",
        "\n",
        "    Returns:\n",
        "        dict with pass/fail for each criterion\n",
        "    \"\"\"\n",
        "    print(\"\\nüîç Production Readiness Check...\")\n",
        "\n",
        "    checks = {\n",
        "        'latency_p95': {\n",
        "            'pass': self.results['latency']['total_p95_ms'] < 500,\n",
        "            'value': f\"{self.results['latency']['total_p95_ms']:.2f} ms\",\n",
        "            'threshold': '< 500 ms'\n",
        "        },\n",
        "        'quality_bleu': {\n",
        "            'pass': self.results['quality'].get('bleu', 0) > 0.3,\n",
        "            'value': f\"{self.results['quality'].get('bleu', 0):.4f}\",\n",
        "            'threshold': '> 0.3'\n",
        "        },\n",
        "        'bertscore_f1': {  # NEW\n",
        "            'pass': self.results['quality'].get('bertscore_f1', 0) > 0.7,\n",
        "            'value': f\"{self.results['quality'].get('bertscore_f1', 0):.4f}\",\n",
        "            'threshold': '> 0.7'\n",
        "        },\n",
        "        'bi_encoder_similarity': {  # NEW\n",
        "            'pass': self.results['quality'].get('bi_encoder_similarity', 0) > 0.7,\n",
        "            'value': f\"{self.results['quality'].get('bi_encoder_similarity', 0):.4f}\",\n",
        "            'threshold': '> 0.7'\n",
        "        },\n",
        "        'memory_usage': {\n",
        "            'pass': self.results['resource'].get('memory_peak_mb', float('inf')) < 8000,\n",
        "            'value': f\"{self.results['resource'].get('memory_peak_mb', 0):.2f} MB\",\n",
        "            'threshold': '< 8000 MB'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    for check_name, check_data in checks.items():\n",
        "        status = \"‚úÖ PASS\" if check_data['pass'] else \"‚ùå FAIL\"\n",
        "        print(f\"{status} | {check_name}: {check_data['value']} (threshold: {check_data['threshold']})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    overall_pass = all([c['pass'] for c in checks.values()])\n",
        "    if overall_pass:\n",
        "        print(\"\\nüéâ MODEL IS PRODUCTION READY!\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  MODEL NEEDS OPTIMIZATION BEFORE PRODUCTION\")\n",
        "\n",
        "    return checks\n"
      ],
      "metadata": {
        "id": "WPGip5TJvcOm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluator instance\n",
        "evaluator = ProductionModelEvaluator(\n",
        "    model=ft_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100,\n",
        "    device='cuda'\n",
        ")"
      ],
      "metadata": {
        "id": "BIw_kd7owD38"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. RUN LATENCY EVALUATION"
      ],
      "metadata": {
        "id": "75mRwHJewLuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare latency test data from the first 100 sample\n",
        "latency_test_data = [question for question in dataset[:100]['question']]\n",
        "print(\"First question:\",latency_test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwHHg99NwPap",
        "outputId": "5f2a8a1e-550e-4cdd-b017-b7898c5a3784"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First question: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on 200 runs starting with 10 warmup runs for the gpu\n",
        "latency_results = evaluator.measure_latency(\n",
        "    test_inputs=latency_test_data,\n",
        "    num_runs=100,\n",
        "    warmup_runs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBUMFUtBxVOz",
        "outputId": "96ce3ce1-be05-4378-a21e-7bbb220eba5f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Measuring Latency...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Latency Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:32<00:00,  5.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Latency Results:\n",
            "\n",
            "  Total Latency:\n",
            "    Mean: 5725.98 ms\n",
            "    P95: 6877.57 ms\n",
            "\n",
            "  TTFT (Time to First Token):\n",
            "    Mean: 687.12 ms\n",
            "    P95: 825.31 ms\n",
            "\n",
            "  TGT (Token Generation Time):\n",
            "    Mean: 5038.86 ms\n",
            "\n",
            "  TPOT (Time Per Output Token):\n",
            "    Mean: 56.44 ms\n",
            "    P95: 61.21 ms\n",
            "\n",
            "  ‚ùå SLOW - Needs optimization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results\n",
        "print(f\"\\nüìä Latency Results Summary:\")\n",
        "print(f\"  Mean Latency: {latency_results['total_mean_ms']:.2f} ms\")\n",
        "print(f\"  P95 Latency: {latency_results['total_p95_ms']:.2f} ms\")\n",
        "print(f\"  TTFT Mean: {latency_results['ttft_mean_ms']:.2f} ms\")\n",
        "print(f\"  TPOT Mean: {latency_results['tpot_mean_ms']:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQfhM6P-xfpf",
        "outputId": "3bd90566-44fd-4ed8-e770-916005c6c145"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Latency Results Summary:\n",
            "  Mean Latency: 5725.98 ms\n",
            "  P95 Latency: 6877.57 ms\n",
            "  TTFT Mean: 687.12 ms\n",
            "  TPOT Mean: 56.44 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. RUN THROUGHPUT EVALUATION"
      ],
      "metadata": {
        "id": "3WgLeyFdz0-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare throughput test data\n",
        "throughput_test_data = latency_test_data\n",
        "print(\"First question:\",throughput_test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY8dGJXhz0jv",
        "outputId": "d2a65b5f-a5a1-461a-ba66-08893257e17f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First question: What area did NVIDIA initially focus on before expanding to other computationally intensive fields?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "throughput_results = evaluator.measure_throughput(\n",
        "    test_inputs=throughput_test_data,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff6Rtuts0gQA",
        "outputId": "b6b826f2-77cf-4e94-d0ac-e1d950137de4"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Measuring Throughput...\n",
            "  Batch size 1:\n",
            "    RPS (Requests/sec): 0.16\n",
            "    TPS (Tokens/sec): 15.91\n",
            "  Batch size 4:\n",
            "    RPS (Requests/sec): 0.33\n",
            "    TPS (Tokens/sec): 33.48\n",
            "  Batch size 8:\n",
            "    RPS (Requests/sec): 0.51\n",
            "    TPS (Tokens/sec): 50.76\n",
            "  Batch size 16:\n",
            "    RPS (Requests/sec): 0.69\n",
            "    TPS (Tokens/sec): 68.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results\n",
        "print(f\"\\nüìä Throughput Results Summary:\")\n",
        "for batch_size, metrics in throughput_results.items():\n",
        "    print(f\"  Batch {batch_size}: {metrics['rps']:.2f} req/s, {metrics['tps']:.2f} tokens/s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O54OXtZB0knh",
        "outputId": "133026d5-d034-4a84-d4cd-a6edc77783b6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Throughput Results Summary:\n",
            "  Batch 1: 0.16 req/s, 15.91 tokens/s\n",
            "  Batch 4: 0.33 req/s, 33.48 tokens/s\n",
            "  Batch 8: 0.51 req/s, 50.76 tokens/s\n",
            "  Batch 16: 0.69 req/s, 68.87 tokens/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. RUN RESOURCE USAGE EVALUATION"
      ],
      "metadata": {
        "id": "W0rhmyhz0ieq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare resource usage test sample example\n",
        "resource_usage_test_sample = latency_test_data[0]\n",
        "resource_usage_test_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XKCjKYzZ0om1",
        "outputId": "52125cfd-6b1a-457b-cc66-e6966449df24"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resource_results = evaluator.measure_resource_usage(\n",
        "    test_input=resource_usage_test_sample\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWkFilTP0ojW",
        "outputId": "74307503-6e59-4e8c-a861-8dbc6c6cb3bc"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Measuring Resource Usage...\n",
            "  Memory Before: 5731.42 MB\n",
            "  Memory After: 2669.65 MB\n",
            "  Memory Peak: 5731.65 MB\n",
            "  Memory Used (Peak - Before): 0.24 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results\n",
        "print(f\"\\nüìä Resource Usage Summary:\")\n",
        "print(f\"  Peak Memory: {resource_results.get('memory_peak_mb', 0):.2f} MB\")\n",
        "print(f\"  Memory Used: {resource_results.get('memory_used_mb', 0):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG2YHEUr0ogk",
        "outputId": "3db02a6f-f536-40f8-ac9c-ac191edebbf1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Resource Usage Summary:\n",
            "  Peak Memory: 5731.65 MB\n",
            "  Memory Used: 0.24 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. RUN QUALITY EVALUATION"
      ],
      "metadata": {
        "id": "PZaTu6-Z0vGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare quality test data with 50 samples\n",
        "test_data_quality = [\n",
        "    {'input':question, \\\n",
        "     'expected_output':completion\n",
        "     }\n",
        "    for question, completion in zip(dataset['question'][:50], dataset['answer'][:50])]\n",
        "\n",
        "test_data_quality[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0O2WOrq0wFN",
        "outputId": "01e645be-330d-4988-ddd0-7ca06befd80d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?',\n",
              " 'expected_output': 'NVIDIA initially focused on PC graphics.'}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quality_results = evaluator.evaluate_quality(\n",
        "    test_data=test_data_quality\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muwx69E-0wBt",
        "outputId": "49b26557-3a45-4ac8-f801-ffca4b1b4584"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Evaluating Quality...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [04:44<00:00,  5.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  BLEU Score: 0.3955\n",
            "  Computing Cosine Similarity...\n",
            "  Cosine Similarity: 0.3147\n",
            "  Computing BERTScore...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  BERTScore F1: 0.8662\n",
            "  BERTScore Precision: 0.8482\n",
            "  BERTScore Recall: 0.8857\n",
            "  Computing Bi-Encoder Similarity...\n",
            "  Bi-Encoder Similarity: 0.6621\n",
            "  Exact Match: 0.00%\n",
            "  Partial Match: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results\n",
        "print(f\"\\nüìä Quality Results Summary:\")\n",
        "print(f\"  BLEU Score: {quality_results.get('bleu', 0):.4f}\")\n",
        "print(f\"  Cosine Similarity: {quality_results.get('cosine_similarity', 0):.4f}\")\n",
        "print(f\"  BERTScore F1: {quality_results.get('bertscore_f1', 0):.4f}\")\n",
        "print(f\"  Bi-Encoder Similarity: {quality_results.get('bi_encoder_similarity', 0):.4f}\")\n",
        "print(f\"  Exact Match: {quality_results.get('exact_match', 0):.2%}\")\n",
        "print(f\"  Partial Match: {quality_results.get('partial_match', 0):.2%}\")\n",
        "\n",
        "# Print sample predictions\n",
        "print(f\"\\nüìã Sample Predictions:\")\n",
        "for i, sample in enumerate(quality_results['samples'][:3]):\n",
        "    print(f\"\\n  Sample {i+1}:\")\n",
        "    print(f\"    Input: {sample['input'][:80]}...\")\n",
        "    print(f\"    Expected: {sample['expected'][:80]}...\")\n",
        "    print(f\"    Predicted: {sample['predicted'][:80]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLhQh0kX063A",
        "outputId": "a812a24c-578e-4d67-b05a-41ee9548c638"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Quality Results Summary:\n",
            "  BLEU Score: 0.3955\n",
            "  Cosine Similarity: 0.3147\n",
            "  BERTScore F1: 0.8662\n",
            "  Bi-Encoder Similarity: 0.6621\n",
            "  Exact Match: 0.00%\n",
            "  Partial Match: 0.00%\n",
            "\n",
            "üìã Sample Predictions:\n",
            "\n",
            "  Sample 1:\n",
            "    Input: What area did NVIDIA initially focus on before expanding to other computationall...\n",
            "    Expected: NVIDIA initially focused on PC graphics....\n",
            "    Predicted: \n",
            "\n",
            "A) Mobile devices\n",
            "B) Desktop computers\n",
            "C) Gaming\n",
            "D) Scientific computing\n",
            "\n",
            "Answ...\n",
            "\n",
            "  Sample 2:\n",
            "    Input: What are some of the recent applications of GPU-powered deep learning as mention...\n",
            "    Expected: Recent applications of GPU-powered deep learning include recommendation systems,...\n",
            "    Predicted: \n",
            "\n",
            "## Answer:\n",
            "NVIDIA has been at the forefront of GPU-powered deep learning, with...\n",
            "\n",
            "  Sample 3:\n",
            "    Input: What significant invention did NVIDIA create in 1999?...\n",
            "    Expected: NVIDIA invented the GPU in 1999....\n",
            "    Predicted: \n",
            "\n",
            "NVIDIA created the GeForce 256, the first GPU with pixel shading capability, i...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. GENERATE COMPREHENSIVE REPORT"
      ],
      "metadata": {
        "id": "SwtMxiPW1Dio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_path = './finetuned_phi3_evaluation_report.html'\n",
        "evaluator.generate_report(save_path=report_path)\n",
        "\n",
        "print(f\"\\n‚úÖ HTML Report saved to: {report_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr-Boq2U1Akw",
        "outputId": "21382068-8535-449f-9746-970c4180720b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìù Generating comprehensive report...\n",
            "‚úÖ Report saved to: ./finetuned_phi3_evaluation_report.html\n",
            "\n",
            "‚úÖ HTML Report saved to: ./finetuned_phi3_evaluation_report.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. PRODUCTION READINESS CHECK"
      ],
      "metadata": {
        "id": "PPgKE8eE1NcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "readiness_results = evaluator.production_readiness_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd45vs601AhM",
        "outputId": "27b0586b-3793-46e6-af98-7771521a1eaf"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Production Readiness Check...\n",
            "\n",
            "============================================================\n",
            "‚ùå FAIL | latency_p95: 6877.57 ms (threshold: < 500 ms)\n",
            "‚úÖ PASS | quality_bleu: 0.3955 (threshold: > 0.3)\n",
            "‚úÖ PASS | bertscore_f1: 0.8662 (threshold: > 0.7)\n",
            "‚ùå FAIL | bi_encoder_similarity: 0.6621 (threshold: > 0.7)\n",
            "‚úÖ PASS | memory_usage: 5731.65 MB (threshold: < 8000 MB)\n",
            "============================================================\n",
            "\n",
            "‚ö†Ô∏è  MODEL NEEDS OPTIMIZATION BEFORE PRODUCTION\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save results into wandb"
      ],
      "metadata": {
        "id": "6OK9OH4c1-Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "HOvuUVlt9xYG",
        "outputId": "1e773b9b-3a79-44f9-d611-420f86650a48"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251224_104558-fh3vzg5z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized/runs/fh3vzg5z' target=\"_blank\">eager-energy-2</a></strong> to <a href='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized' target=\"_blank\">https://wandb.ai/eslam3012mohamed-not-yet/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized/runs/fh3vzg5z' target=\"_blank\">https://wandb.ai/eslam3012mohamed-not-yet/uncategorized/runs/fh3vzg5z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized/runs/fh3vzg5z?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x78a450f6ab70>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log all evaluator results\n",
        "import pandas as pd\n",
        "\n",
        "wandb.log({\n",
        "    # Latency - all metrics\n",
        "    \"latency/total_mean_ms\": evaluator.results['latency']['total_mean_ms'],\n",
        "    \"latency/total_median_ms\": evaluator.results['latency']['total_median_ms'],\n",
        "    \"latency/total_p95_ms\": evaluator.results['latency']['total_p95_ms'],\n",
        "    \"latency/total_p99_ms\": evaluator.results['latency']['total_p99_ms'],\n",
        "    \"latency/std_ms\": evaluator.results['latency']['std_ms'],\n",
        "    \"latency/min_ms\": evaluator.results['latency']['min_ms'],\n",
        "    \"latency/max_ms\": evaluator.results['latency']['max_ms'],\n",
        "    \"latency/ttft_mean_ms\": evaluator.results['latency']['ttft_mean_ms'],\n",
        "    \"latency/ttft_median_ms\": evaluator.results['latency']['ttft_median_ms'],\n",
        "    \"latency/ttft_p95_ms\": evaluator.results['latency']['ttft_p95_ms'],\n",
        "    \"latency/tgt_mean_ms\": evaluator.results['latency']['tgt_mean_ms'],\n",
        "    \"latency/tgt_median_ms\": evaluator.results['latency']['tgt_median_ms'],\n",
        "    \"latency/tpot_mean_ms\": evaluator.results['latency']['tpot_mean_ms'],\n",
        "    \"latency/tpot_median_ms\": evaluator.results['latency']['tpot_median_ms'],\n",
        "    \"latency/tpot_p95_ms\": evaluator.results['latency']['tpot_p95_ms'],\n",
        "\n",
        "    # Throughput - all batch sizes\n",
        "    \"throughput/batch_1_rps\": evaluator.results['throughput'][1]['rps'],\n",
        "    \"throughput/batch_1_tps\": evaluator.results['throughput'][1]['tps'],\n",
        "    \"throughput/batch_4_rps\": evaluator.results['throughput'][4]['rps'],\n",
        "    \"throughput/batch_4_tps\": evaluator.results['throughput'][4]['tps'],\n",
        "    \"throughput/batch_8_rps\": evaluator.results['throughput'][8]['rps'],\n",
        "    \"throughput/batch_8_tps\": evaluator.results['throughput'][8]['tps'],\n",
        "    \"throughput/batch_16_rps\": evaluator.results['throughput'][16]['rps'],\n",
        "    \"throughput/batch_16_tps\": evaluator.results['throughput'][16]['tps'],\n",
        "    \"throughput/best_rps\": max([v['rps'] for v in evaluator.results['throughput'].values()]),\n",
        "    \"throughput/best_tps\": max([v['tps'] for v in evaluator.results['throughput'].values()]),\n",
        "\n",
        "    # Quality - all metrics\n",
        "    \"quality/bleu\": evaluator.results['quality']['bleu'],\n",
        "    \"quality/cosine_similarity\": evaluator.results['quality']['cosine_similarity'],\n",
        "    \"quality/bertscore_precision\": evaluator.results['quality']['bertscore_precision'],\n",
        "    \"quality/bertscore_recall\": evaluator.results['quality']['bertscore_recall'],\n",
        "    \"quality/bertscore_f1\": evaluator.results['quality']['bertscore_f1'],\n",
        "    \"quality/bi_encoder_similarity\": evaluator.results['quality']['bi_encoder_similarity'],\n",
        "    \"quality/exact_match\": evaluator.results['quality']['exact_match'],\n",
        "    \"quality/partial_match\": evaluator.results['quality']['partial_match'],\n",
        "\n",
        "    # Resource - all metrics\n",
        "    \"resource/memory_before_mb\": evaluator.results['resource']['memory_before_mb'],\n",
        "    \"resource/memory_after_mb\": evaluator.results['resource']['memory_after_mb'],\n",
        "    \"resource/memory_peak_mb\": evaluator.results['resource']['memory_peak_mb'],\n",
        "    \"resource/memory_used_mb\": evaluator.results['resource']['memory_used_mb'],\n",
        "})\n",
        "\n",
        "# Log predictions table\n",
        "samples_df = pd.DataFrame(evaluator.results['quality']['samples'])\n",
        "wandb.log({\"predictions\": wandb.Table(dataframe=samples_df)})"
      ],
      "metadata": {
        "id": "TQXeLv8b1_q5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End wandb tracing\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "YO9nvZ7N2A8T",
        "outputId": "6a1614fa-c04e-4d67-8f09-4d05d87ad967"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>latency/max_ms</td><td>‚ñÅ</td></tr><tr><td>latency/min_ms</td><td>‚ñÅ</td></tr><tr><td>latency/std_ms</td><td>‚ñÅ</td></tr><tr><td>latency/tgt_mean_ms</td><td>‚ñÅ</td></tr><tr><td>latency/tgt_median_ms</td><td>‚ñÅ</td></tr><tr><td>latency/total_mean_ms</td><td>‚ñÅ</td></tr><tr><td>latency/total_median_ms</td><td>‚ñÅ</td></tr><tr><td>latency/total_p95_ms</td><td>‚ñÅ</td></tr><tr><td>latency/total_p99_ms</td><td>‚ñÅ</td></tr><tr><td>latency/tpot_mean_ms</td><td>‚ñÅ</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>latency/max_ms</td><td>18137.65669</td></tr><tr><td>latency/min_ms</td><td>1313.02953</td></tr><tr><td>latency/std_ms</td><td>2579.27504</td></tr><tr><td>latency/tgt_mean_ms</td><td>6222.37054</td></tr><tr><td>latency/tgt_median_ms</td><td>5863.78845</td></tr><tr><td>latency/total_mean_ms</td><td>6763.44623</td></tr><tr><td>latency/total_median_ms</td><td>6373.68309</td></tr><tr><td>latency/total_p95_ms</td><td>12547.97881</td></tr><tr><td>latency/total_p99_ms</td><td>14830.65121</td></tr><tr><td>latency/tpot_mean_ms</td><td>70.55164</td></tr><tr><td>+27</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-energy-2</strong> at: <a href='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized/runs/fh3vzg5z' target=\"_blank\">https://wandb.ai/eslam3012mohamed-not-yet/uncategorized/runs/fh3vzg5z</a><br> View project at: <a href='https://wandb.ai/eslam3012mohamed-not-yet/uncategorized' target=\"_blank\">https://wandb.ai/eslam3012mohamed-not-yet/uncategorized</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251224_104558-fh3vzg5z/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvsr5-r1if21"
      },
      "source": [
        "# Merge, Save, and Package the Final Model\n",
        "\n",
        "Our work is not complete until the model is saved and ready for deployment. The fine-tuning process created lightweight LoRA \"adapter\" weights, which are separate from the original base model. For easy, portable inference, we need to merge these adapters back into the base model to create a single, unified set of weights.\n",
        "\n",
        "This final step effectively \"bakes in\" our specialized financial knowledge.\n",
        "\n",
        "1.  **Merge and Unload**: We call `model.merge_and_unload()`. This powerful Unsloth function performs two actions:\n",
        "    * **Merges** the trained LoRA weights directly into the base model's attention and MLP layers.\n",
        "    * **Unloads** the PEFT wrapper, returning a standard Hugging Face `PreTrainedModel` object. This new object is a complete, standalone model that doesn't require the `peft` library for inference.\n",
        "2.  **Save the Model**: We use the standard `save_pretrained` method to save our new, merged model. We specify `safe_serialization=True` to use the modern and secure `safetensors` format.\n",
        "3.  **Save the Tokenizer**: Crucially, we also save the tokenizer in the same directory. The model weights and the tokenizer are a pair; you need both to correctly run inference.\n",
        "\n",
        "With the first light of dawn approaching over the Giza plateau, our final, expert financial analyst model is now serialized to disk, ready to be uploaded, shared, and deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4m0kiwdif22",
        "outputId": "72dfd3ec-356f-4199-e749-46c420e0fda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging LoRA adapter with base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create directory for the complete fine-tuned model\n",
        "save_directory = \"Data/complete_finetuned_model\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "print(\"Merging LoRA adapter with base model...\")\n",
        "# Merge the LoRA adapter with the base model\n",
        "merged_model = ft_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E36ZHA75if23",
        "outputId": "876f80ed-dfce-4ad3-9828-98960fa0e0c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving merged model and tokenizer...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Data/complete_finetuned_model/tokenizer_config.json',\n",
              " 'Data/complete_finetuned_model/special_tokens_map.json',\n",
              " 'Data/complete_finetuned_model/chat_template.jinja',\n",
              " 'Data/complete_finetuned_model/tokenizer.model',\n",
              " 'Data/complete_finetuned_model/added_tokens.json',\n",
              " 'Data/complete_finetuned_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Saving merged model and tokenizer...\")\n",
        "\n",
        "# Save the complete merged model\n",
        "merged_model.save_pretrained(\n",
        "    save_directory,\n",
        "    safe_serialization=True,  # Use safetensors format (recommended)\n",
        "    max_shard_size=\"2GB\"      # Split large models into 2GB chunks\n",
        ")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(save_directory)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}